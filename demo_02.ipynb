{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Ch2. Linear Classifier\n",
    "----\n",
    "This is the sample code of TU-ETP-AD1062 Machine Learning Fundamentals.\n",
    "\n",
    "For more information, please refer to:\n",
    "https://sites.google.com/view/tu-ad1062-mlfundamentals/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.svm\n",
    "import sklearn.linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from mlfund.dataset import Gaussian\n",
    "from mlfund.dataset import GaussianParam\n",
    "from mlfund.plot import Plot2D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1. Generate random dataset in 2-Dimension\n",
    "----\n",
    "The demo here generate two groups of 2-dimension data which are normally distributed as following:\n",
    "1. Generate 200 training data `X_train`, with corresponded label `y_train`\n",
    "2. Generate 100 testing data `X_test`, with corresponded label `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Training data and plot it\n",
    "np.random.seed(0)\n",
    "\n",
    "params_train = []\n",
    "\n",
    "param = GaussianParam()\n",
    "param.mean = [-1, 2.5]\n",
    "param.cov = [[1, 5], [0, 1]]\n",
    "param.N = 100\n",
    "params_train.append(param)\n",
    "\n",
    "param = GaussianParam()\n",
    "param.mean = [1, -2.5]\n",
    "param.cov = [[1, 5], [0, 1]]\n",
    "param.N = 100\n",
    "params_train.append(param)\n",
    "\n",
    "X_train, y_train = Gaussian.generate(params_train)\n",
    "\n",
    "plot = Plot2D()\n",
    "plot.scatter(X_train, y_train)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate testing data\n",
    "params_test = []\n",
    "\n",
    "param = GaussianParam()\n",
    "param.mean = [-0.1, 2.5]\n",
    "param.cov = [[1, 5], [0, 1]]\n",
    "param.N = 50\n",
    "params_test.append(param)\n",
    "\n",
    "param = GaussianParam()\n",
    "param.mean = [0.1, -2.5]\n",
    "param.cov = [[1, 5], [0, 1]]\n",
    "param.N = 50\n",
    "params_test.append(param)\n",
    "\n",
    "X_test, y_test = Gaussian.generate(params_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2. Perceptron\n",
    "----\n",
    "The demo here trains the model by Perceptron algorithm with `X_train`, then predict the testing data by `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfPLA = sklearn.linear_model.Perceptron(max_iter=100, tol=None)\n",
    "clfPLA.fit(X_train, y_train)\n",
    "\n",
    "y_test_predict = clfPLA.predict(X_test)\n",
    "\n",
    "print(\"Training data:\")\n",
    "plot = Plot2D()\n",
    "plot.scatter(X_train, y_train)\n",
    "plot.classifierContour(X_train, y_train, clfPLA)\n",
    "plot.show()\n",
    "\n",
    "print(\"Testing data:\")\n",
    "print(\"MCE = %2.3f\" % sklearn.metrics.zero_one_loss(y_test, y_test_predict))\n",
    "plot = Plot2D()\n",
    "plot.scatter(X_test, y_test)\n",
    "plot.classifierContour(X_test, y_test, clfPLA)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3. c-Support Vector Machine (c-SVC)\n",
    "----\n",
    "The demo here trains the model by SVM with `X_train`, then predict the testing data by `X_test`\n",
    "Notice that:\n",
    "1. The number of support vectors is output via the attribute of `clfSVC.support_vectors_`\n",
    "2. The support vectors are drawn via the wrapped function `mlfund.scatterSV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfSVC = sklearn.svm.SVC(C=1, kernel='linear')\n",
    "clfSVC.fit(X_train, y_train)\n",
    "\n",
    "y_test_predict = clfSVC.predict(X_test)\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(\"#SV = %d\" % len(clfSVC.support_vectors_))\n",
    "plot = Plot2D()\n",
    "plot.scatter(X_train, y_train)\n",
    "plot.scatterCSVC(clfSVC)\n",
    "plot.classifierContour(X_train, y_train, clfSVC)\n",
    "plot.show()\n",
    "\n",
    "print(\"Testing data:\")\n",
    "print(\"MCE = %2.3f\" % sklearn.metrics.zero_one_loss(y_test, y_test_predict))\n",
    "plot = Plot2D()\n",
    "plot.scatter(X_test, y_test)\n",
    "plot.classifierContour(X_test, y_test, clfSVC)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 4. Support Vector Machine - More \"crowded\" case\n",
    "----\n",
    "The demo here use the same settings of the c-SVM model, but learning from a more crowded data. One could adjust the value of `C` to observe the support vectors being relaxed by slack variables\n",
    "* The larger `C`, the less support vectors (due to the more penalty of $\\xi_i$), but the smaller margin size\n",
    "* The smaller `C`, the more support vectors (due to the less penalty of $\\xi_i$), but the larger margin size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Training data and plot it\n",
    "np.random.seed(0)\n",
    "\n",
    "params_train = []\n",
    "\n",
    "param = GaussianParam()\n",
    "param.mean = [-0.3, 2]\n",
    "param.cov = [[1, 5], [0, 1]]\n",
    "param.N = 100\n",
    "params_train.append(param)\n",
    "\n",
    "param = GaussianParam()\n",
    "param.mean = [0.3, -2]\n",
    "param.cov = [[1, 5], [0, 1]]\n",
    "param.N = 100\n",
    "params_train.append(param)\n",
    "\n",
    "X_train, y_train = Gaussian.generate(params_train)\n",
    "\n",
    "# Generate testing data\n",
    "params_test = []\n",
    "\n",
    "param = GaussianParam()\n",
    "param.mean = [-0.3, 2]\n",
    "param.cov = [[1, 5], [0, 1]]\n",
    "param.N = 50\n",
    "params_test.append(param)\n",
    "\n",
    "param = GaussianParam()\n",
    "param.mean = [0.3, -2]\n",
    "param.cov = [[1, 5], [0, 1]]\n",
    "param.N = 50\n",
    "params_test.append(param)\n",
    "\n",
    "X_test, y_test = Gaussian.generate(params_test)\n",
    "\n",
    "# Try different settings here!\n",
    "# clfSVC = sklearn.svm.SVC(C=0.1, kernel='linear')\n",
    "# clfSVC = sklearn.svm.SVC(C=1, kernel='linear')\n",
    "# clfSVC = sklearn.svm.SVC(C=10, kernel='linear')\n",
    "# clfSVC = sklearn.svm.SVC(C=100, kernel='linear')\n",
    "# clfSVC = sklearn.svm.SVC(C=1000, kernel='linear')\n",
    "# ...\n",
    "\n",
    "clfSVC = sklearn.svm.SVC(C=1000, kernel='linear')\n",
    "clfSVC.fit(X_train, y_train)\n",
    "\n",
    "y_test_predict = clfSVC.predict(X_test)\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(\"#SV = %d\" % len(clfSVC.support_vectors_))\n",
    "plot = Plot2D()\n",
    "plot.scatter(X_train, y_train)\n",
    "plot.scatterCSVC(clfSVC)\n",
    "plot.classifierContour(X_train, y_train, clfSVC)\n",
    "plot.show()\n",
    "\n",
    "print(\"Testing data:\")\n",
    "print(\"MCE = %2.3f\" % sklearn.metrics.zero_one_loss(y_test, y_test_predict))\n",
    "plot = Plot2D()\n",
    "plot.scatter(X_test, y_test)\n",
    "plot.classifierContour(X_test, y_test, clfSVC)\n",
    "plot.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
